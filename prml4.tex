\documentclass[a4paper]{jsarticle}
\usepackage{amsmath,amssymb}
\usepackage{bm}

\include{notation}

\newcommand{\bmm}{\bm {m}}

\begin{document}

\title{PRMLの4章のための数学}
\author{サイボウズ・ラボ 光成滋生}

\maketitle

\section{概要}
この文章は『パターン認識と機械学習』（以下PRML）の4章を理解するために必要な数学の一部です.
間違い, 質問などございましたら{\tt herumi@nifty.com}または{\tt twitterID:herumi}までご連絡ください.

\section{行列の微分の復習}
$A=(a_{ij})$とかいた.
$$(AB)_{ij}=\sum_k a_{ik} b_{kj}.$$
$$\tr(A)=\sum_i a_{ii}$$
$$\trans{A}=(a_{ji})$$
などを思い出しておく.

さて$A$, $B$を適当な行列として
$$
\diff{A}\tr(AB)=\trans{B}
$$
なぜなら, 
$$
(\diff{A}\tr(AB))_{ij}=\diff{a_{ij}}\sum_{s,t} a_{st} b_{ts} = b_{ji}.
$$
ここで
$\diff{a_{ij}} a_{st} = \delta_{is} \delta_{jt}$を使った. つまり添え字$s$, $t$が走るときに, $s=i$, $t=j$のときのみが生き残るというわけである.

慣れるためにもう一つやっておこう.
$$
\diff{A}\tr(AB\trans{A})=A(B+\trans{B}).
$$
なぜなら,
\begin{eqnarray*}
\diff{a_{ij}} \tr(AB\trans{A})
 &=& \diff{a_{ij}} \sum_{s,t,u} a_{st} b_{tu} a_{su}\\
 &=& \sum_{s,t,u} b_{tu}  \diff{a_{ij}} (a_{st} a_{su})\\
 &=& \sum_{s,t,u} b_{tu} (\delta_{is} \delta_{jt} a_{su} + a_{st} \delta_{is} \delta_{ju})\\
 &=& \sum_{u} b_{ju} a_{iu} + \sum_{t} b_{tj} a_{it}\\
 &=& \sum_{u} a_{iu} b_{ju} + \sum_{t} a_{it} b_{tj}\\
 &=& (A\trans{B})_{ij} + (AB)_{ij}\\
 &=& (A(B + \trans{B}))_{ij}.
\end{eqnarray*}

\section{多クラス}
$K$個の線形関数を使った$K$クラス識別を考える.
$$
y_k(x)=\trans{w_k}x+w_{k0}.
$$
ここで$w_k$は重みベクトル, $w_{k0}$はバイアスパラメータでスカラー, $x$が分類したい入力パラメータでベクトルである.

クラス分類を次の方法で定義する:
$x$に対して, ある$k$が存在し, 全ての$j\neq k$にたいして$y_k(x) > y_j(x)$であるとき$x$はクラス$C_k$に割り当てるとする.

これはwell-definedである.
つまり
\begin{itemize}
\item  （一意性）$x$が二つの異なるクラス$C_k$に$C_k'$に属することはない. なぜならそういう$k$, $k'$があったとすると$y_k(x) > y_k'(x) > y_k(x)$となり矛盾するから.
\item （存在性）$x$が与えられたとき$\{y_k(x)\}$の最大値$m$を与える$k_0$がその候補である. もしも
$m=y_k(x)$となる$k$が複数個存在（$k_1$, $k_2$）したとすると, クラス分類はできないが, そういう$x$の集合は$\{x|y_{k_1}(x)=y_{k_2}(x)\}$の部分集合となり, 通常次元が落ちる. つまり無理できるぐらいしかない.
\end{itemize}

上記で分類されたクラス$C_k$に属する空間は凸領域となる.
すなわち
$x$, $x'$を$C_K$の点とすると, 任意の$\lambda \in [0, 1]$に対して$x''=\lambda x + (1-\lambda) x'$も$C_k$に属する.

なぜなら$x$, $x' \in C_k$より任意の$j\neq k$にたいして$y_k(x) > y_j(x)$, $y_k(x') > y_j(x')$.
$y_k(x)$は$x$について線形なので$\lambda \geq 0$, $1-\lambda \geq 0$より
$$
y_k(x'') = \lambda y_k(x) + (1-\lambda) y_k(x') > \lambda y_j(x) + (1-\lambda) y_j(x') = y_j(x'')
$$
が成り立つからである.

凸領域は単連結（simply connected）である. つまりその領域の中に空洞は無い. 任意の凸領域の2点を結ぶ線分が凸領域に入ることから直感的には明らかであろう.

\section{分類における最小二乗}
前節では重みベクトル$w_{k0}$を別扱いしたが, $\tilde{w}_k=\trans{(w_{k0}, \trans{w_k})}$,
$\tilde{x}=\trans{(1,\trans{x})}$と1次元増やすと$y_k(x)=\trans{\tilde{w}}\tilde{x}$とかける.
面倒なので$\tilde{x}$を$x$と置き換えてしまおう.

さらにまとめて
$y(x)=\trans{W}x$としよう. $x$, $y$はベクトル, $W$は行列である.

二乗誤差関数
$$
E_D(W)=\frac{1}{2}\tr(\trans{(XW-T)}(XW-T))
$$
を最小化する$W$を求めよう.

\begin{eqnarray*}
\diff{w_{ij}}E_D(W)
 &=& \frac{1}{2}\diff{w_{ij}}\sum_{s,t} ((XW-T)_{st})^2\\
 &=& \sum_{s,t} (XW-T)_{st} \diff{w_{ij}}(XW-T)_{st}\\
 &=& \sum_{s,t} (XW-T)_{st} \diff{w_{ij}}(\sum_u x_{su}w_{ut})\\
 &=& \sum_{s,t,u} (XW-T)_{st} x_{su}\delta_{iu}\delta_{jt}\\
 &=& \sum_s (XW-T)_{sj} x_{si}\\
 &=& \sum_s (\trans{X})_{is} (XW-T)_{sj}\\
 &=& (\trans{X}(XW-T))_{ij}.
\end{eqnarray*}
よって
$$\diff{W}E_D(W)=\trans{X}(XW-T).$$
$=0$とおいて
$\trans{X}XW = \trans{X}T$より
$$W=(\trans{X}X)^{-1}\trans{X}T.$$

\section{フィッシャーの線形判別}
まず$D$次元のベクトル$x$の入力に対して$y=\trans{w}x$で1次元に射影する.
$y \ge w_0$なら$C_1$, そうでないなら$C_2$に分類する.
$C_1$の点が$N_1$個, $C_2$の点が$N_2$個とする.
$C_i$の点の平均は
$$\bmm_i=\frac{1}{N_i}\sum_{n \in C_i} x_n.$$

$m_i=\trans{w}\bm{m_i}$として, $|w|^2=\sum_i w_i^2=1$の制約下で
$$
m_2 - m_1=\trans{w}(\bmm_2-\bmm_1)
$$
を最大化してみよう.
$$
f(w)=\trans{w}(\bmm_2-\bmm_1)+\lambda(1-|w|^2)
$$
とおくと
$$
\diff{w}f=\bmm_2-\bmm_1-2\lambda w = 0.
$$
よって
$$
w=\frac{1}{2\lambda}(\bmm_2-\bmm_1) \propto (\bmm_2-\bmm_1).
$$
$$
\diff{\lambda}f=1-|w|^2=0
$$
より$|w|=1$. ただしこの手法ではそれぞれのクラスの重心$\bmm_1$と$\bmm_2$とだけで$w$の向きが決まってしまい,
場合によっては二つのクラスの射影が大きく重なってうまく分離できないことがある.
そこでクラス間の重なりを最小にするように分散も加味してみる.

クラス$C_k$から射影されたデータのクラス内の分散を
$$
y_n=\trans{w}x_n, s_k^2=\sum_{n\in C_k}(y_n-m_k)^2
$$
で定義し, 全データに対する分散を$s_1^2+s_2^2$とする.

フィッシャーの判別基準は
$$
J(w)=\frac{(m_2-m_1)^2}{s_1^2+s_2^2}
$$
で定義される. この定義を書き直してみよう.
$$
S_B=(\bmm_2-\bmm_1)\trans{(\bmm_2-\bmm_1)},
$$
$$
S_W=\sum_{n \in C_1}(x_n-\bmm_1)\trans{(x_n-\bmm_1)}+\sum_{n \in C_2}(x_n-\bmm_2)\trans{(x_n-\bmm_2)}
$$
とする. $S_B$をクラス間共分散行列, $S_W$を総クラス内共分散行列という.

$\trans{w}S_B w=\trans{w}(\bmm_2-\bmm_1)\trans{(\bmm_2-\bmm_1)}w=(m_2-m_1)^2$.
$$
\trans{w}S_W w=\trans{w}\sum_{n \in C_1}(x_n-\bmm_1)\trans{(x_n-\bmm_1)}w+\trans{w}\sum_{n \in C_2}(x_n-\bmm_2)\trans{(x_n-\bmm_2)}w
=\sum_{n\in C_1} (y_n-m_1)^2+\sum_{n\in C_2}(y_n-m_2)^2
$$
より
$$
J(w)=\frac{\trans{w}S_B w}{\trans{w} S_W w}.
$$
これが最大となる$w$の値を求めてみよう. 大きさはどうでもよくて向きが重要である.
$$
\diff{w}J(w)=(2 (S_B w)(\trans{w} S_W w) - 2 (\trans{w} S_B w) (S_W w))/(\trans{w}S_W w)^2=0.
$$
よって
$$
(\trans{w}S_B w)S_w w = (\trans{w} S_W w)S_B w.
$$
$S_B w = (\bmm_2-\bmm_1)(\trans{(\bmm_2-\bmm_1)}w)\propto (\bmm_2-\bmm_1)$だから
$$
w \propto S_W^{-1}S_B w \propto S_W^{-1}(\bmm_2-\bmm_1)
$$
のときに$J(w)$が最大となる. これをフィッシャーの線形判別（linear discriminant）という.

\section{最小二乗との関連}
\begin{itemize}
\item 最小二乗法：目的変数の値の集合にできるだけ近いように
\item フィッシャーの判別基準：クラスの分離を最大化するように
\end{itemize}
2クラスの分類のときは最小二乗の特別な場合がフィッシャーの判別基準であることをみる.
フィッシャーの判別基準が, 最小二乗と関係があることが分かるとそちらの議論が使えていろいろ便利なことがある.

クラス$C_i$に属するパターンの個数を$N_i$として全体を$N=N_1+N_2$とする.
クラス$C_1$に対する目的変数値を$N/N_1$, クラス$C_2$に対する目的変数値を$-N/N_2$とする.

この条件下で二乗和誤差
$$E=\frac{1}{2}\sum_{n=1}^N(\trans{w}x_n+w_0-t_n)^2$$
を最大化してみよう.
$$
\diff{w_0}E=\sum(\trans{w}x_n+w_0-t_0)=0
$$
より$\bmm=(1/N)\sum x_n$とおくと$N \trans{w}m+N w_0 - \sum t_n=0$.
$$\sum t_n=N1(N/N_1) + N_2(-N/N_2)=0$$
より$w_0=-\trans{w}\bmm$. また
$$
\sum(\trans{w}x_n)x_n=\sum(\trans{x_n}w)x_n=\sum(x_n \trans{x_n})w.
$$
$$
\sum w_0 x_n = N w_0 \bmm = -N(\trans{w}\bmm)\bmm=-N(\bmm \trans{\bmm})w.
$$
$$
\sum t_n x_n = \sum_{n\in C_1} t_n x_n + \sum_{n\in C_2} t_n x_n
= N/N_1(N_1 \bmm_1)+(-N/N_2)(N2 \bmm_2)=N(\bmm_1-\bmm_2).
$$
よって
$$
\diff{w}E=\sum(\trans{w}x_n + w_0-t_n)x_n=0
$$
を使うと
$$
\sum(x_n \trans{x_n})w=N(\bmm \trans{\bmm})w+N(\bmm_1-\bmm_2).
$$
これらの式を使って$S_w$を計算する.
\begin{eqnarray*}
S_W &=& \sum_{n\in C_1} x_n\trans{x_n}-2\sum_{C_1} x_n \trans{\bmm_1}+\sum_{C_1} \bmm_1\trans{\bmm_1} + \sum_{C_2} x_n\trans{x_n} - 2\sum_{C_2} x_n\trans{\bmm_2} + \sum_{C_2} \bmm_2 \trans{\bmm_2}\\
 &=&\sum x_n\trans{x_n}-N_1 \bmm_1 \trans{\bmm_1}-N_2 \bmm_2\trans{\bmm_2}\\
 &=&N(\bmm \trans{\bmm})w+N(\bmm_1-\bmm_2)-N_1 \bmm_1 \trans{\bmm_1}-N_2 \bmm_2\trans{\bmm_2}.
\end{eqnarray*}
よって
$$
(S_W+\frac{N_1 N_2}{N}S_B)w=N(\bmm_1-\bmm_2)+\{N\bmm \trans{\bmm}-N_1\bmm_1 \trans{\bmm_1}-N_2 \bmm_2 \trans{\bmm_2}+\frac{N_1 N_2}{N} (\bmm_1 -\bmm_2)\trans{(\bmm_1-\bmm_2)}\}w.
$$

$\{\}$内が0であることを示す（めんどうなので$\bmm_i$を$m_i$と略する）.
\begin{eqnarray*}
\{\}
&=& \frac{1}{N}(N_1 m_1+N_2 m_2)\trans{(N_1 m_1+N_2 m_2)}-N_1 m_1\trans{m_1}-N_2 m_2\trans{m_2}+\frac{N_1 N_2}{N}(m_1 \trans{m_2}+m_2 \trans{m_2})\\
&=& (\frac{N_1^2}{N}-N_1+\frac{N_1 N_2}{N})m_1\trans{m_1}+(\frac{2}{N} N_1 N_2 - \frac{2}{N}N_1 N_2)m_1\trans{m_2}+(\frac{N_2^2}{N}-N_2+\frac{N_1 N_2}{N})m_2 \trans{m_2}.
\end{eqnarray*}
$$
\frac{N_1^2}{N}-N_1+\frac{N_1 N_2}{N}=\frac{N_1}{N}(N_1-N+N_2)=0,
$$
$$
\frac{N_2^2}{N}-N_2+\frac{N_1 N_2}{N}=\frac{N_2}{N}(N_2-N+N_1)=0.
$$
よって
$$
(S_W+\frac{N_1 N_2}{N}S_B)w=N(\bmm_1-\bmm_2).
$$
$S_Bw\propto(\bmm_2 -\bmm_1)$なので$w \propto S_W^{-1}(\bmm_2 - \bmm_1)$.
\end{document}
