\documentclass[a4paper]{jsarticle}
\usepackage{amsmath,amssymb}
\usepackage{bm}

\include{notation}

\newcommand{\wji}{w_{ji}^{(1)}}
\newcommand{\wkj}{w_{kj}^{(2)}}

\begin{document}

\title{PRMLの9章の数式の補足}
\author{サイボウズ・ラボ 光成滋生}

\maketitle

\section{概要}
この文章は『パターン認識と機械学習』（以下PRML）の9章の式変形を一部埋めたものです.
間違い, 質問などございましたら{\tt herumi@nifty.com}または{\tt twitterID:herumi}までご連絡ください.
面倒なので特に紛らわしいと思わない限り$\bm{x}$を$x$と書いたりします. また対数尤度関数を$F$と書くことが多いです.

\section{復習}
よく使ういくつかの式を書いておく. どれも今までに既に示したものである.\\
{\tt https://github.com/herumi/prml/raw/master/prml2.pdf},\\
{\tt https://github.com/herumi/prml/raw/master/prml3.pdf}
を参照.

\subsection{行列の公式}
\begin{eqnarray*}
&& \quads{A}{x}=\tr(Ax\trans{x}),\\
&& \diff{A}\log|A|=\trans{(A^{-1})},\\
&& \diff{x}\log|A|=\tr(A^{-1}\diff{x}A),\\
&& \diff{A}\tr(A^{-1}B) = -\trans{(A^{-1}BA^{-1})}.
\end{eqnarray*}

\subsection{微分}
関数$f$に対して対数関数の微分は
$$
(\log f)'=\frac{f'}{f}.
$$
よって逆に
$$
f'=f \cdot (\log f)'.
$$
ガウス分布など対数の微分が分かりやすいときによく使う.

\subsection{ガウス分布}
$$
\calN = \calN(x|\mu,\Sigma)=\frac{1}{(2\pi)^{D/2}}|\Sigma|^{-1/2}\exp(-\frac{1}{2}\quads{{\Sigma^{-1}}}{(x-\mu)}).
$$
期待値と分散について
\begin{eqnarray*}
&& E[x]=\mu, \\
&& \cov[x]=\Sigma, \\
&& E[\outp{x}]=\outp{\mu}+\Sigma, \\
&& E[\inp{x}]=\inp{\mu}+\tr(\Sigma).
\end{eqnarray*}


\section{混合ガウス分布}
離散的な潜在変数を用いた混合ガウス分布の定式化.
$K$次元2値確率変数$z$を考える（どれか一つの成分のみが1であとは0）.
つまり
$$
\sum_k z_k = 1.
$$
$z$の種類は$K$個である. $0 \le \pi_k \le 1$という係数を用いて
$$
p(z_k=1)=\pi_k
$$
という確率分布を与える.
$$
p(z)=\prod_k \pi_k^{z_k}.
$$
$$
p(x|z_k=1)=\calN(x|\mu_k,\Sigma_k)
$$
なので
$$
P(x|z)=\prod_k \calN(x|\mu_k,\Sigma_k)^{z_k}.
$$
これらを合わせて
\begin{eqnarray*}
p(x) &=& \sum_z p(z)p(x|z) \\
 &=& \sum_z \prod_k (\pi_k \calN(x|\mu_k,\Sigma_k))^{z_k}\\
 && \text{$z_k$はどれか一つのみが1（そのとき$\pi_k$）であとは0なので}\\
 &=& \sum_{k} \pi_k \calN(x|\mu_k,\Sigma_k).
\end{eqnarray*}

$x$が与えられたときの$z$の条件付き確率$p(z_k=1|x)$を$\gamma(z_k)$とする.
$$
\gamma(z_k)=\frac{p(z_k=1)p(x|z_k=1)}{\sum_j p(z_j=1)p(x|z_j=1)}
 = \frac{\pi_k \calN(x|\mu_k,\Sigma_k)}{\sum_j \pi_j \calN(x|\mu_j,\Sigma_j)}.
$$
これを混合要素$k$が観測値$x$に対する負担率という.

\section{混合ガウス分布のEMアルゴリズム}
混合ガウス分布において観測したデータ集合を$\trans{X}=\{x_1,\ldots, x_N\}$,
対応する潜在変数を$\trans{Z}=\{z_1,\ldots, z_N\}$とする.
$X$は$N \times D$行列で$Z$は$N \times K$行列.

対数尤度関数の最大点の条件をもとめる.
$$
F=\log p(X|\vpi,\mu,\Sigma)=\sum_{n=1}^N \log (\sum_{j=1}^K \pi_j \calN(x_n|\mu_j,\Sigma_j))
$$
とする.
$$
\diff{\mu} \log \calN(x|\mu,\Sigma)
= \diff{\mu} (-\frac{1}{2}\quads{{\Sigma^{-1}}}{{(x-\mu)}})
= \Sigma^{-1}(x-\mu)
$$
より
$$
\diff{\mu} \calN = \calN \cdot (\diff{\mu} \log \calN) = \calN \cdot \Sigma^{-1}(x-\mu).
$$
$\calN_{nk}=\calN(x_n|\mu_k,\Sigma_k)$とおいて
\begin{eqnarray*}
\diff{\mu_k}F
 &=& \sum_n \frac{\pi_k \diff{\mu_k}\calN_{nk}}{\sum_j \pi_j \calN_{nj}} \\
 &=& \sum_n (\frac{\pi_k \calN_{nk}}{\sum_j \pi_j \calN_{nj}})\diff{\mu_k}\log \calN_{nk} \\
 &=& \sum_n \gamma(z_{nk}) \diff{\mu_k}\log \calN_{nk} \\
 &=& \Sigma_k^{-1}(\sum_n \gamma(z_{nk})(x_n-\mu_k))=0.
\end{eqnarray*}
よって
$$
\sum_n \gamma(z_{nk})x_n - (\sum_n \gamma(z_{nk}))\mu_k=0.
$$
$$
N_k=\sum_n \gamma(z_{nk})
$$
とおくと
$$
\mu_k=\frac{1}{N_k}\sum_n \gamma(z_{nk})x_n.
$$
これは$\mu_k$が$X$の重みつき平均であることを示している.

次に$\Sigma_k$に関する微分を考える.
$$
\calN = \calN(x|\mu,\Sigma)
$$
のとき
$$
\log \calN = -\frac{D}{2}\log(2\pi)-\frac{1}{2}\log |\Sigma| - \frac{1}{2}\tr(\Sigma^{-1}\outp{(x-\mu)})
$$
なので$\trans{\Sigma}=\Sigma$だから
$$
\diff{\Sigma} (\log \calN)
= -\frac{1}{2}(\Sigma^{-1})
  +\frac{1}{2}(\Sigma^{-1}(x-\mu)\trans{(x-\mu)}\Sigma^{-1}).
$$
よって$\mu_k$の微分と同様にして
\begin{eqnarray*}
\diff{\Sigma_k}F
 &=& \sum_n \gamma(z_{nk}) \diff{\Sigma_k} \log \calN_{nk}\\
 &=&\sum_n \gamma(z_{nk})(-\frac{1}{2}(\Sigma_k^{-1})
        + \frac{1}{2}(\Sigma_k^{-1}\outp{(x_n-\mu_k)}\Sigma_k^{-1})) = 0.
\end{eqnarray*}
よって
$$
\sum_n \gamma(z_{nk})(I-\outp{(x_n-\mu_k)}\Sigma_k^{-1})=0.
$$
$$
\Sigma_k=\frac{1}{N_k}\sum_n \gamma(z_{nk})\outp{(x_n-\mu_k)}.
$$

最後に$\pi_k$に関する微分を考える.
$\sum_k \pi_k=1$の制約を入れる.
$$
G=F+\lambda(\sum_k \pi_k-1)
$$
とすると
$$
\diff{\pi_k}{G}
=\sum_n \frac{\calN_{nk}}{\sum_j \pi_j\calN_{nj}}+\lambda=\sum_n \gamma(z_{nk})/\pi_k+\lambda=N_k/\pi_k+\lambda=0.
$$
つまり
$$
N_k = -\lambda \pi_k.
$$
よって
$$
N=\sum_k N_k=\sum_k (-\lambda \pi_k) = -\lambda.
$$
よって
$$
\pi_k=\frac{N_k}{-\lambda}=\frac{N_k}{N}.
$$
\section{混合ガウス分布再訪}
$$
p(z)=\prod_k \pi_k^{z_{k}},
$$
$$
p(x|z)=\prod_k \calN(x|\mu_k,\Sigma_k)^{z_k}
$$
より
\begin{eqnarray*}
F=\log p(X,Z|\mu,\Sigma,\vpi)
 &=& \log (\prod_{n,k} \pi_k^{z_{nk}}\calN(x_n|\mu_k,\Sigma_k)^{z_{nk}})\\
 &=& \sum_{n,k} z_{nk}(\log \pi_k + \log \calN_{nk})).
\end{eqnarray*}
$z_n$は$(0, 0,\ldots, 1, 0, \ldots, 0)$の形で$\sum_k \pi_k=1$の制約条件を入れると上式の微分を考えると
$$
G=F+\lambda(\sum_k \pi_k-1)
$$
として
$$
\diff{\pi_k}G=\sum_n z_{nk}\frac{1}{\pi_k}+\lambda=(\sum_n z_{nk})/\pi_k + \lambda=0.
$$
よって
$$
\pi_k = -\frac{1}{\lambda}\sum_n z_{nk}.
$$
$$
\sum_k \pi_k=-\frac{1}{\lambda}\sum_{n,k}z_{nk}=-\frac{N}{\lambda}=1.
$$
よって$\lambda=-N$.
つまり
$$
\pi_k=\frac{1}{N}\sum_n z_{nk}.
$$
完全データ集合についての対数尤度関数の最大化は解けるが, 潜在変数が分からない場合の不完全データに関する対数尤度関数の最大化は困難.
この場合は潜在変数の事後分布に関する完全データ尤度関数の期待値を考える.
$$
p(Z|X,\mu,\Sigma,\vpi)
 = \frac{p(X,Z|\mu,\Sigma,\pi)}{p(X|\mu,\Sigma,\vpi)}
 \propto \prod_{n,k} (\pi_k \calN_{nk})^{z_{nk}}.
$$
$$
E[z_{nk}]=\frac{\sum_{z_n} z_{nk} \prod_j (\pi_j \calN_{nj})^{z_{nj}}}
               {\sum_{z_n} \prod_j(\pi_j \calN_{nj})^{z_{nj}}}
         =\frac{\pi_k \calN_{nk}}{\sum_j \pi_j \calN_{nj}} = \gamma(z_{nk}).
$$
よって
$$
F=E_Z[\log p(X,Z|\mu,\Sigma,\vpi)]=\sum_{n,k}\gamma(z_{nk})(\log \pi_k + \log \calN_{nk}).
$$
まずパラメータ$\mu$, $\Sigma$, $\vpi$を適当に決めて負担率$\gamma(z_{nk})$を求め,
それをfixして$\mu_k$, $\Sigma_k$, $\pi_k$について$F$を最大化.
今までと同様にできる.
$F'=F+\lambda(\sum_k \pi_k-1)$として
$$
\diff{\pi_k}F'=\sum_n \gamma(z_{nk})(1/\pi_k)+\lambda=0
$$
より
$$
\sum_n \gamma(z_{nk})=\lambda \pi_k.
$$
$$
\sum_{n,k}\gamma(z_{nk})=-\lambda(\sum_k \pi_k)=-\lambda=N
$$
より
$$
\pi_k=\frac{1}{N}\sum_n \gamma(z_{nk})=\frac{N_k}{N}.
$$
$$
\diff{\mu_k}F=\sum_n\gamma(z_{nk})(-\Sigma_k^{-1}(x_n-\mu_k))=\Sigma_k^{-1}(\sum_n \gamma(z_{nk})x_n-(\sum_n \gamma(z_{nk}))\mu_k)=0.
$$
よって
$$
\mu_k=\frac{1}{N_k}\sum_n \gamma(z_{nk})x_n.
$$
$$
\diff{\Sigma_k}F=\sum_n \gamma(z_{nk}) \diff{\Sigma_k}\log \calN_{nk}=0
$$
として同様（流石に略）.
\section{$K$-meansとの関連}
式(9.43)は不正確. $E$ではなく$\epsilon E$を考えないと(9.43)の右辺にはならない.
式(9.40)を$E$とおく.
$$
E=\sum_{n,k} \gamma(z_{nk})(\log \pi_k + \log \calN(x_n|\mu_k,\Sigma_k)).
$$
$\epsilon E$に
$$
\calN(x|\mu_k,\Sigma_k)=\frac{1}{(2\pi \epsilon)^{D/2}}\exp(-\frac{1}{2\epsilon}||x-\mu_k||^2)
$$
を代入する.
$$
\epsilon E = \sum_{n,k} \gamma(z_{nk})(\epsilon \log \pi_k - \frac{D}{2}\epsilon \log (2\pi \epsilon) - \frac{1}{2}||x_n-\mu_k||^2).
$$
$\epsilon \rightarrow 0$で
$$
\gamma(z_{nk}) \rightarrow r_{nk},
$$
$$
\epsilon \log \pi_k \rightarrow 0,
$$
$$
\epsilon \log (2\pi \epsilon) \rightarrow 0
$$
より
$$
\epsilon E \rightarrow -\frac{1}{2}\sum_{n,k} r_{nk} ||x_n-\mu_k||^2 = -J.
$$
よって期待完全データ対数尤度の最大化は$J$の最小化と同等.

\section{混合ベルヌーイ分布}
$x=\trans{(x_1,\ldots,x_D)}$, $\mu=\trans{(\mu_1, \ldots, \mu_D)}$とする.
$$
p(x|\mu)=\prod_{i=1}^D \mu_i^{x_i}(1-\mu_i)^{(1-x_i)}.
$$
$E[x]=\mu$は容易に分かる.
$$
E[x_i x_j]=
\begin{cases}
\mu_i \mu_j (i \ne j)\\
\mu_i (i = j).
\end{cases}
$$
よって
$$
\cov[x]_{ij}=E[\outp{(x-\mu)}]_{ij}=E[x_i x_j]-(\outp{\mu})_{ij}=(\mu_i - \mu_i^2)\delta_{ij}
$$
より
$$
\cov[x]=\diag(\mu_i(1-\mu_i)).
$$

$\mu=\{\mu_1, \ldots, \mu_K\}$, $\vpi=\{\pi_1, \ldots, \pi_K\}$として次の混合分布を考えよう.
$$
p(x|\mu_k)=\prod_i \mu_{ki}^{x_i}(1-\mu_{ki})^{(1-x_i)}.
$$
$$
E[x]=\int x p(x|\mu)\,dx=\sum_k \pi_k \int x p(x|\mu_k)\,dx=\sum_k \pi_k E_k[x]=\sum_k \pi_k \mu_k.
$$
$$
E_k[\outp{x}]=\cov_k[x]+\outp{\mu_k}=\Sigma_k+\outp{\mu_k}
$$
より
$$
\cov[x]=E[\outp{(x-E[x])}]=E[\outp{x}]-\outp{E[x]}=\sum_k \pi_k(\Sigma_k+\outp{\mu_k})-\outp{E[x]}.
$$
データ集合$X=\{x_1, \ldots, x_N\}$が与えられたとき, 対数尤度関数は
$$
\log p(X|\mu,\vpi)=\sum_n \log (\sum_k \pi_k p(x_n|\mu_k)).
$$
対数の中に和があるので解析的に最尤解をもとめられない. EMアルゴリズムを使う.
$x$に対応する潜在変数を$z=\trans{(z_1,\ldots,z_K)}$を導入する.
どれか一つのみ1でその他は0のベクトルである.
$z$の事前分布を
$$
p(z|\pi)=\prod_k \pi_k^{z_k}
$$
とする. $z$が与えられたときの条件付き確率は
$$
p(x|z,\mu)=\prod_k p(x|\mu_k)^{z_k}.
$$
$$
p(x,z|\mu,\vpi)=p(x|z,\mu)p(z|\vpi)=\prod_k(\pi_k p(x|\mu_k))^{z_k}.
$$
よって
$$
p(x|\mu,\vpi)=\sum_{z} p(x,z|\mu,\vpi)=\sum_k \pi_k p(x|\mu_k).
$$
完全データ対数尤度関数は$X=\{x_n\}$, $Z=\{z_n\}$として
\begin{eqnarray*}
\log p(X,Z|\mu,\vpi)
 &=& \sum_{n,k} z_{nk}(\log \pi_k + \sum_i x_{ni} \log \mu_{ki}+(1-x_{ni})\log (1-\mu_{ki}))\\
 &=& \sum_{n,k} z_{nk} A_{nk} \text{とおく}.
\end{eqnarray*}

続く

\end{document}
