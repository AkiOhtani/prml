\documentclass[a4paper]{jsarticle}
\usepackage{amsmath,amssymb}
\usepackage{bm}

\include{notation}

\begin{document}

\title{PRMLの3章のための数学}
\author{サイボウズ・ラボ 光成滋生}

\maketitle

\section{概要}
この文章は『パターン認識と機械学習』（以下PRML）の3章を理解するために必要な数学をまとめてみたものです.

いくつかの定理は証明せずに認めますが, 可能な限りself-containedであることを目指してみました.
概ねPRMLに従ってますが, 違う方法をとっているところもあります.

間違い, 質問などございましたら, {\tt herumi@nifty.com}または{\tt twitterID:herumi}までご連絡ください.

\section{最小二乗法}

\subsection{微分の復習}

$\bm{x}$, $\bm{y}$を縦ベクトルとして
$$\diff{\bm{x}} (\trans{\bm{x}}\bm{y}) = \bm{y}.$$
$$\diff{\bm{y}} (\trans{\bm{x}}\bm{y}) = \bm{x}.$$

ここで$\diff{\bm{x}}$は$\diff{x_i}$を縦に並べた縦ベクトルとする.
2章でも述べたが$\diff{\bm{x}}$を$\nabla$と書くこともあるがPRMLでは場所によって縦ベクトル（3.22）だったり,
横ベクトル（3.13）だったりする. 常に縦ベクトルとしたほうが混乱は少ない.

\subsection{誤差関数の最小化}

$$f(\bm{w})=\sum_{n=1}^N\{t_n - \trans{\bm{w}}\phi(\bm{x}_n)\}^2+\lambda \trans{\bm{w}}\bm{w}$$

とする. ここで$\bm{w}$と$\phi(\bm{x}_n)$は$M$次元縦ベクトルである.

$$\trans{\Phi}=(\phi(\bm{x}_1) \cdots \phi(\bm{x}_N))$$
とおく. $\Phi$は$N$行$M$列の行列である. $f(\bm{w})$を$w$で微分しよう.

$$\diff{\bm{w}}f(\bm{w})
= 2\sum_{n=1}^N\{t_n - \trans{\bm{w}}\phi(\bm{x}_n)\}(-\phi(\bm{x}_n)) +2\lambda \bm{w}.$$

一般に縦ベクトル$\bm{x}$, $\bm{y}$に対して
$$(\trans{\bm{x}}\bm{y})\bm{y}
 =(\trans{\bm{y}}{\bm{x}})\bm{y}
 =\bm{y}(\trans{\bm{y}}\bm{x})
 =(\bm{y}\trans{\bm{y}})\bm{x}
$$だから$\bm{t}=\trans{(t_1, \ldots, t_N)}$とおくと

\begin{eqnarray*}
\frac{1}{2}\diff{\bm{w}}f(\bm{w})
 &=& -\sum_n t_n \phi(\bm{x}_n) + \sum_n (\phi(\bm{x}_n)\trans{\phi(\bm{x}_n)})\bm{w} + \lambda \bm{w}\\
 &=& -\trans{\Phi} \bm{t} + \trans{\Phi}\Phi \bm{w} + \lambda \bm{w}\\
 &=& -\trans{\Phi} \bm{t} + (\trans{\Phi}\Phi + \lambda I)\bm{w}=0.	
\end{eqnarray*}

よって$\det(\lambda I + \trans{\Phi}\Phi)\ne0$のとき

$$\bm{w}_{\text{ML}}=(\lambda I + \trans{\Phi}\Phi)^{-1}\trans{\Phi}\bm{t}$$
が最尤解. $\bm{y}=\Phi \bm{w}$が予測値である.

\subsection{正射影}

前節で$\lambda = 0$のときを考える.
$$\bm{y}=\Phi(\trans{\Phi}\Phi)^{-1}\trans{\Phi}\bm{t}$$
となる. ここでこの式の幾何学的な解釈を考えてみよう.

$\Phi=(a_1 \cdots a_M)$と縦ベクトルの集まりで表す.
$N-M$個のベクトル$b_1$, $\ldots$, $b_{N-M}$を追加して,
$\{a_1, \ldots, a_N, b_1, \ldots, b_{N-M}\}$全体で$N$次元ベクトル空間の基底であるようにとる.
その際$b_i$を$a_j$と直交するようにとれる.
$$\trans{a_i}b_j=0.$$

さて$X=\Phi(\trans{\Phi}\Phi)^{-1}\trans{\Phi}$とおくと, $X\Phi = \Phi$.
これは$Xa_i = a_i$を意味する.
つまり$X$は$a_1, \ldots, a_M$で生成される部分空間$V$=$\langle a_1, \ldots, a_M \rangle$の点を動かさない.
また$b_j$のとりかたから$Xb_j=0$も成り立つ.
つまり$X$は部分空間$\langle b_1, \ldots, b_{N-M} \rangle$の点を0につぶす.

二つ合わせると, $X$は任意の点を部分空間$V$方向につぶす写像, つまり$V$への正射影写像と解釈できる.

式で書くと任意の点$\bm{t}$を$\bm{t} = \sum_i s_i a_i + \sum_i t_i b_j$と表したとすると,
$$\bm{y}=X\bm{t}=\sum_i s_i a_i$$
となる. 係数だけを取り出してみると
$$X:(s_1, \ldots, s_N, t_1, \ldots, t_{N-M}) \rightarrow (s_1, \ldots, s_N, 0, \ldots, 0).$$

\subsection{行列での微分}

$x$を$n$次元ベクトル, $A$を$m$行$n$列として$y=Ax$とおく.
$$f(A) = ||y||^2=\trans{(Ax)}Ax$$
を$A$で微分してみよう.
$$\trans{(Ax)}Ax=\sum_s (Ax)_s (Ax)_s = \sum_s (\sum_t a_{st}x_t) (\sum_u a_{su}x_u) = \sum_{s,t,u} x_t x_u a_{st}a_{su}.$$
よって
\begin{eqnarray*}
\diff{a_{ij}}f(A) &=& \sum_{s,t,u} x_t x_u ((\diff{a_{ij}}a_{st}) a_{su} + a_{st} \diff{a_{ij}}a_{su})\\
 &=& \sum_{s,t,u}x_t x_u(\delta_{is}\delta_{jt}a_{su} + a_{st} \delta_{is}\delta_{ju})\\
 &=& (\sum_u x_j x_u a_{iu}) + (\sum_t x_t x_j a_{it})\\
 &=& 2\sum_u x_j x_u a_{iu}\\
 &=& 2x_j (Ax)_i\\
 &=& 2(Ax\trans{x})_{ij}.
\end{eqnarray*}
よって
$$\diff{A}||Ax||^2 = 2Ax\trans{x}.$$

\subsection{Woodburyの逆行列の公式}

行列$A$, $B$, $C$, $D$について
$$(A+BD^{-1}C)^{-1}=A^{-1}-A^{-1}B(D+CA^{-1}B)^{-1}CA^{-1}$$
が成り立つ.

（証明）
\begin{eqnarray*}
A^{-1}B(D+CA^{-1}B)^{-1}CA^{-1} &=& A^{-1}B((DB^{-1}A+C)A^{-1}B)^{-1}CA^{-1}\\
 &=&(DB^{-1}A+C)^{-1}CA^{-1}\\
 &=&(DB^{-1}(A+BD^{-1}C))^{-1}CA^{-1}\\
 &=&(A+BD^{-1}C)^{-1}BD^{-1}CA^{-1}
\end{eqnarray*}
よって
\begin{eqnarray*}
\text{左辺} &=& (I-(A+BD^{-1}C)^{-1}BD^{-1}C)A^{-1}\\
 &=& (A+BD^{-1}C)^{-1}((A+BD^{-1}C)-BD^{-1}C)A^{-1}\\
 &=& \text{右辺}.
\end{eqnarray*}

特に, $A$が$n$次正方行列で$B$を$n$次縦ベクトル$\bm{x}$, $C=\trans{\bm{x}}$, $D$を$n$次単位行列とすると
\begin{equation}\label{inv_A_xx}
(A+\bm{x}\trans{\bm{x}})^{-1}=A^{-1}-\frac{(A^{-1}\bm{x})(\trans{\bm{x}}A^{-1})}{1+\trans{\bm{x}}A^{-1}\bm{x}}
\end{equation}
が成り立つ.

\subsection{正定値対称行列}
$n$次元実対称行列$A$はある直行行列$P$を用いて常に対角化可能であった.
$$P^{-1}AP=\diag(\lambda_1, \cdots, \lambda_n).$$
全ての固有値が正であるとき$A$を正定値といい, $A > 0$とかく.
全ての固有値が正または0であるとき, 半正定値といい, $A \ge 0$とかく.


任意の実ベクトル$\bm{x}$について$\bm{y}=P\bm{x}$とおくと$\bm{x}$が$\RR^n$の全ての点をとるとき$\bm{y}$も全ての点を渡る.
$$\quadf{A}{x}=\sum_i \lambda_i y_i^2$$
なので$A \ge 0$ならば$\quadf{A}{x} \ge 0$. $A>0$のときは等号が成り立つのは$\bm{x}=0$のときのみである.

逆に任意の$\bm{x}$について$\quadf{A}{x} \ge 0$とすると,
$\bm{y}$として単位ベクトル$\bm{e_i}$を考えれば$\lambda_i \ge 0$. つまり$A \ge 0$.

更に等号は$\bm{x}=0$のときに限るためには$\lambda_i > 0$. つまり$A>0$であることが分かる.
まとめると
$$ A \ge 0 \iff \quadf{A}{x} \ge 0. \mbox{ for } \forall \bm{x} \in \RR$$
$$ A>0 \iff \mbox{上記不等式が成り立ち, 更に等号は }\bm{x}=0\mbox{ のときに限る.}$$

この同値性から$A>0$, $B>0$のとき$A+B>0$. また$A^{-1}>0$も分かる.

また実ベクトル$\bm{v}$に対して$A=\bm{v}\trans{\bm{v}}$とおくと,
$A$は実対称であり, 任意の$\bm{x}$に対して
$$\quadf{A}{x}=(\trans{\bm{v}}\bm{x})^2 \ge 0$$
なので$A \ge 0$.

\subsection{予測分布の分散}
$S_N^{-1}=S_0^{-1} + \beta \trans{\Phi_N}\Phi_N$としたときの予測分布の分散
$$\sigma_N^2=\frac{1}{\beta}+\trans{\phi} S_N \phi$$
を考える. $\beta>0$であり, $S_0$は共分散行列なので実正定値であることに注意する.
まず計画行列$\Phi_N$は$N$が一つ増える毎に1行増える. $v$を$M$次元縦ベクトルとして
$$\trans{\Phi_{N+1}}=\bigl(\trans{\Phi_N}\text{ } v\bigr)$$
としよう.
すると
$$
S_{N+1}^{-1}
 =S_0^{-1} + \beta(\trans{\Phi_N}\Phi_N + v\trans{v})
 = S_N^{-1} + \beta v\trans{v}.
$$
行列$A=\beta v\trans{v}$は正定値であり, $S_N$に関して帰納法を使うと全ての$S_N$は正定値であることが分かる.

公式\ref{inv_A_xx}を使って
\begin{eqnarray*}
\sigma_{N+1}^2&=&\frac{1}{\beta} + \trans{\phi}(S_N^{-1} + \beta v\trans{v})^{-1}\phi\\
 &=& \frac{1}{\beta} + \trans{\phi}(S_N - \frac{(S_N v)(\trans{v} S_N)}{1+\trans{v}S_Nv})\phi\\
 &=& \sigma_N^2 - z
\end{eqnarray*}
ここで$S_N$は対称なので
\begin{eqnarray*}
z &=& \trans{\phi}\frac{(S_N v)(\trans{v} S_N)}{1+\trans{v}S_Nv}\phi\\
  &=& \frac{1}{1+\trans{v}S_Nv} (\trans{v}S_N\phi)^2.
\end{eqnarray*}
$S_N$は正定値なので任意の$v$に対して$\trans{v}S_Nv \ge 0$.
よって $z \ge 0$となり
$$\sigma_{N+1}^2 \le \sigma_N^2.$$

\end{document}
