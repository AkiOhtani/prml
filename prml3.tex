\documentclass[a4paper]{jsarticle}
\usepackage{amsmath,amssymb}
\usepackage{bm}
\usepackage[dvips]{graphicx}

\include{notation}

\begin{document}

\title{PRMLの3章のための数学}
\author{サイボウズ・ラボ 光成滋生}

\maketitle

\section{概要}
この文章は『パターン認識と機械学習』（以下PRML）の3章を理解するために必要な数学をまとめてみたものです.
間違い, 質問などございましたら{\tt herumi@nifty.com}または{\tt twitterID:herumi}までご連絡ください.

\section{最小二乗法}

\subsection{微分の復習}
$\bm{x}$, $\bm{y}$を縦ベクトルとして
$$\diff{\bm{x}} (\trans{\bm{x}}\bm{y}) = \bm{y}.$$
$$\diff{\bm{y}} (\trans{\bm{x}}\bm{y}) = \bm{x}.$$

ここで$\diff{\bm{x}}$は$\diff{x_i}$を縦に並べた縦ベクトルとする.
2章でも述べたが$\diff{\bm{x}}$を$\nabla$と書くこともあるがPRMLでは場所によって縦ベクトル（3.22）だったり,
横ベクトル（3.13）だったりする. 常に縦ベクトルとしたほうが混乱は少ない.

\subsection{誤差関数の最小化}

$$f(\bm{w})=\sum_{n=1}^N\{t_n - \trans{\bm{w}}\phi(\bm{x}_n)\}^2+\lambda \trans{\bm{w}}\bm{w}$$
とする. ここで$\bm{w}$と$\phi(\bm{x}_n)$は$M$次元縦ベクトルである.
$$\trans{\Phi}=(\phi(\bm{x}_1) \cdots \phi(\bm{x}_N))$$
とおく. $\Phi$は$N$行$M$列の行列である. $f(\bm{w})$を$w$で微分しよう.
$$\diff{\bm{w}}f(\bm{w})
= 2\sum_{n=1}^N\{t_n - \trans{\bm{w}}\phi(\bm{x}_n)\}(-\phi(\bm{x}_n)) +2\lambda \bm{w}.$$
一般に縦ベクトル$\bm{x}$, $\bm{y}$に対して
$$(\trans{\bm{x}}\bm{y})\bm{y}
 =(\trans{\bm{y}}{\bm{x}})\bm{y}
 =\bm{y}(\trans{\bm{y}}\bm{x})
 =(\bm{y}\trans{\bm{y}})\bm{x}
$$だから$\bm{t}=\trans{(t_1, \ldots, t_N)}$とおくと
\begin{eqnarray*}
\frac{1}{2}\diff{\bm{w}}f(\bm{w})
 &=& -\sum_n t_n \phi(\bm{x}_n) + \sum_n (\phi(\bm{x}_n)\trans{\phi(\bm{x}_n)})\bm{w} + \lambda \bm{w}\\
 &=& -\trans{\Phi} \bm{t} + \trans{\Phi}\Phi \bm{w} + \lambda \bm{w}\\
 &=& -\trans{\Phi} \bm{t} + (\trans{\Phi}\Phi + \lambda I)\bm{w}=0.	
\end{eqnarray*}
よって$\det(\lambda I + \trans{\Phi}\Phi)\ne0$のとき
$$\bm{w}_{\text{ML}}=(\lambda I + \trans{\Phi}\Phi)^{-1}\trans{\Phi}\bm{t}$$
が最尤解. $\bm{y}=\Phi \bm{w}$が予測値である.

\subsection{正射影}

前節で$\lambda = 0$のときを考える.
$$\bm{y}=\Phi(\trans{\Phi}\Phi)^{-1}\trans{\Phi}\bm{t}$$
となる. ここでこの式の幾何学的な解釈を考えてみよう.

$\Phi=(a_1 \cdots a_M)$と縦ベクトルの集まりで表す.
$N-M$個のベクトル$b_1$, $\ldots$, $b_{N-M}$を追加して,
$\{a_1, \ldots, a_N, b_1, \ldots, b_{N-M}\}$全体で$N$次元ベクトル空間の基底であるようにとる.
その際$b_i$を$a_j$と直交するようにとれる.
$$\trans{a_i}b_j=0.$$

さて$X=\Phi(\trans{\Phi}\Phi)^{-1}\trans{\Phi}$とおくと, $X\Phi = \Phi$.
これは$Xa_i = a_i$を意味する.
つまり$X$は$a_1, \ldots, a_M$で生成される部分空間$V$=$\langle a_1, \ldots, a_M \rangle$の点を動かさない.
また$b_j$のとりかたから$Xb_j=0$も成り立つ.
つまり$X$は部分空間$\langle b_1, \ldots, b_{N-M} \rangle$の点を0につぶす.

二つ合わせると, $X$は任意の点を部分空間$V$方向につぶす写像, つまり$V$への正射影写像と解釈できる.
式で書くと任意の点$\bm{t}$を$\bm{t} = \sum_i s_i a_i + \sum_i t_i b_j$と表したとすると,
$$\bm{y}=X\bm{t}=\sum_i s_i a_i$$
となる. $\bm{t}$から$\bm{y}$への変換を係数だけを使って書いてみると
$$X:(s_1, \ldots, s_N, t_1, \ldots, t_{N-M}) \rightarrow (s_1, \ldots, s_N, 0, \ldots, 0).$$
これを見ると正射影のニュアンスがより明確になる.

\subsection{行列での微分}

$x$を$n$次元ベクトル, $A$を$m$行$n$列として$y=Ax$とおく.
$$f(A) = ||y||^2=\trans{(Ax)}Ax$$
を$A$で微分してみよう.
$$\trans{(Ax)}Ax=\sum_s (Ax)_s (Ax)_s = \sum_s (\sum_t a_{st}x_t) (\sum_u a_{su}x_u) = \sum_{s,t,u} x_t x_u a_{st}a_{su}.$$
よって
\begin{eqnarray*}
\diff{a_{ij}}f(A) &=& \sum_{s,t,u} x_t x_u ((\diff{a_{ij}}a_{st}) a_{su} + a_{st} \diff{a_{ij}}a_{su})\\
 &=& \sum_{s,t,u}x_t x_u(\delta_{is}\delta_{jt}a_{su} + a_{st} \delta_{is}\delta_{ju})\\
 &=& (\sum_u x_j x_u a_{iu}) + (\sum_t x_t x_j a_{it})\\
 &=& 2\sum_u x_j x_u a_{iu}\\
 &=& 2x_j (Ax)_i\\
 &=& 2(Ax\trans{x})_{ij}.
\end{eqnarray*}
よって
$$\diff{A}||Ax||^2 = 2Ax\trans{x}.$$

\subsection{Woodburyの逆行列の公式}

行列$A$, $B$, $C$, $D$について
$$(A+BD^{-1}C)^{-1}=A^{-1}-A^{-1}B(D+CA^{-1}B)^{-1}CA^{-1}$$
が成り立つ.

（証明）
\begin{eqnarray*}
A^{-1}B(D+CA^{-1}B)^{-1}CA^{-1} &=& A^{-1}B((DB^{-1}A+C)A^{-1}B)^{-1}CA^{-1}\\
 &=&(DB^{-1}A+C)^{-1}CA^{-1}\\
 &=&(DB^{-1}(A+BD^{-1}C))^{-1}CA^{-1}\\
 &=&(A+BD^{-1}C)^{-1}BD^{-1}CA^{-1}
\end{eqnarray*}
よって
\begin{eqnarray*}
\text{左辺} &=& (I-(A+BD^{-1}C)^{-1}BD^{-1}C)A^{-1}\\
 &=& (A+BD^{-1}C)^{-1}((A+BD^{-1}C)-BD^{-1}C)A^{-1}\\
 &=& \text{右辺}.
\end{eqnarray*}

特に, $A$が$n$次正方行列で$B$を$n$次縦ベクトル$\bm{x}$, $C=\trans{\bm{x}}$, $D$を$n$次単位行列とすると
\begin{equation}\label{inv_A_xx}
(A+\bm{x}\trans{\bm{x}})^{-1}=A^{-1}-\frac{(A^{-1}\bm{x})(\trans{\bm{x}}A^{-1})}{1+\trans{\bm{x}}A^{-1}\bm{x}}
\end{equation}
が成り立つ.

\subsection{正定値対称行列}
$n$次元実対称行列$A$はある直行行列$P$を用いて常に対角化可能であった.
$$P^{-1}AP=\diag(\lambda_1, \cdots, \lambda_n).$$
全ての固有値が正であるとき$A$を正定値といい, $A > 0$とかく.
全ての固有値が正または0であるとき, 半正定値といい, $A \ge 0$とかく.

任意の実ベクトル$\bm{x}$について$\bm{y}=P\bm{x}$とおくと$\bm{x}$が$\RR^n$の全ての点をとるとき$\bm{y}$も全ての点を渡る.
$$\quadf{A}{x}=\sum_i \lambda_i y_i^2$$
なので$A \ge 0$ならば$\quadf{A}{x} \ge 0$. $A>0$のときは等号が成り立つのは$\bm{x}=0$のときのみである.

逆に任意の$\bm{x}$について$\quadf{A}{x} \ge 0$とすると,
$\bm{y}$として単位ベクトル$\bm{e_i}$を考えれば$\lambda_i \ge 0$. つまり$A \ge 0$.
更に等号は$\bm{x}=0$のときに限るためには$\lambda_i > 0$. つまり$A>0$であることが分かる.
まとめると
$$ A \ge 0 \iff \lambda_i \ge 0 \mbox{ for } \forall i.$$
$$ A>0 \iff \lambda_i > 0 \mbox{ for } \forall i.$$

この同値性から$A>0$のとき$A^{-1}>0$も分かる. 定義から$A>0$, $B>0$なら$A+B>0$も成り立つ.

また実ベクトル$\bm{v}$に対して$A=\bm{v}\trans{\bm{v}}$とおくと,
$A$は実対称であり, 任意の$\bm{x}$に対して
$$\quadf{A}{x}=(\trans{\bm{v}}\bm{x})^2 \ge 0$$
なので$A \ge 0$.

\subsection{予測分布の分散}
$S_N^{-1}=S_0^{-1} + \beta \trans{\Phi_N}\Phi_N$としたときの予測分布の分散
$$\sigma_N^2=\frac{1}{\beta}+\trans{\phi} S_N \phi$$
を考える. $\beta>0$であり, $S_0$は共分散行列なので実正定値であることに注意する.
まず計画行列$\Phi_N$は$N$が一つ増える毎に1行増える. $\bm{v}_N$（煩雑なので$v$と略記する）を$M$次元縦ベクトルとして
$$\trans{\Phi_{N+1}}=\bigl(\trans{\Phi_N}\text{ } v\bigr)$$
としよう.
すると
$$
S_{N+1}^{-1}
 =S_0^{-1} + \beta(\trans{\Phi_N}\Phi_N + v\trans{v})
 = S_N^{-1} + \beta v\trans{v}.
$$
行列$\beta v\trans{v}$は正定値であり, $S_N$に関して帰納法を使うと全ての$S_N$は正定値であることが分かる.

公式\ref{inv_A_xx}を使って
\begin{eqnarray*}
\sigma_{N+1}^2&=&\frac{1}{\beta} + \trans{\phi}(S_N^{-1} + \beta v\trans{v})^{-1}\phi\\
 &=& \frac{1}{\beta} + \trans{\phi}(S_N - \frac{(S_N v)(\trans{v} S_N)}{1+\trans{v}S_Nv})\phi\\
 &=& \sigma_N^2 - z
\end{eqnarray*}
ここで$S_N$は対称なので
\begin{eqnarray*}
z &=& \trans{\phi}\frac{(S_N v)(\trans{v} S_N)}{1+\trans{v}S_Nv}\phi\\
  &=& \frac{1}{1+\trans{v}S_Nv} (\trans{v}S_N\phi)^2.
\end{eqnarray*}
$S_N$は正定値なので任意の$v$に対して$\trans{v}S_Nv \ge 0$.
よって $z \ge 0$となり
$$\sigma_{N+1}^2 \le \sigma_N^2.$$

帰納法の流れを見ると,
$$
\trans{\Phi_N} = \bigl(\bm{v}_1 \cdots \bm{v}_N \bigr)
$$
とおくと
$$
S_N^{-1}=S_0^{-1}+\beta \sum_{i=1}^N \bm{v}_i\trans{\bm{v}_i}
$$
となることがわかる.
$\bm{v}_i$が基底関数のベクトルに訓練データの値を代入したものであることを考えると, 0ベクトルになることは殆ど無い.
また$N \rightarrow \infty$で0になるわけでもない.
つまりそれらの和はどんどん大きくなる. そういう状況の元では$\trans{\phi}S_N\phi$は0に近づき,
$$\sigma_N^2 \rightarrow \frac{1}{\beta}$$
となる.

\subsection{カルバック距離}

$p(x)$, $q(x)$を恒等的に0ではない確率密度関数とする. つまり$p(x)$, $q(x) \ge 0$.
$$
\mathrm{KL}(p||q)=\int p(x) \log \frac{p(x)}{q(x)}\,dx
$$
をカルバック距離（Kullback-Leibler距離, 相対エントロピー）という.

距離といいつつ, $\mathrm{KL}(p||q)=\mathrm{KL}(q||p)$とは限らないので距離の公理は満たさない.
しかし, $\mathrm{KL}(p||q) \ge 0$であり, $\mathrm{KL}(p||q)=0 \iff p = q$はいえる.
これを示そう.

まず$S(x)=e^{-x}+x-1$について$S(x) \ge 0$であり, $S(x)=0 \iff x = 0$である.

なぜなら$S'(x)=-e^{-x}+1$. $S''(x)=e^{-x} \ge 0$なので$S'(x)$は単調増加. $S'(0) = 0$より$x > 0$なら$S'(x) > 0$, $x<0$なら$S'(x)<0$. つまり$S(x)$は0で最小値0をとる.

$$
\int p(x) S(\log \frac{p(x)}{q(x)})\,dx
 = \int p(x) (\frac{q(x)}{p(x)} + \log \frac{p(x)}{q(x)} - 1)\,dx
 = \mathrm{KL}(p||q) + \int (q(x) - p(x))\,dx = \mathrm{KL}(p||q).
$$
ここで$p$, $q$が確率密度関数なので$\int p(x)\,dx = 1$, $\int q(x)\,dx=1$であることを使った.

この式の左辺の被積分関数は常に0以上. よって$\mathrm{KL}(p||q) \ge 0$.

$\mathrm{KL}(p||q) = 0$ならば殆ど全ての$x$について
$$
p(x) S(\log \frac{p(x)}{q(x)})=0.
$$
$p=0$ではないので殆ど全ての$x$について
$$S(\log \frac{p(x)}{q(x)})=0.$$
$S(x)=0$となる$x$は0のときだけだから,
殆ど全ての$x$について$p(x)=q(x)$.

真のモデル$p(D|M)$があったときに, モデルエビデンス$p(D|M')$との
カルバック距離$\mathrm{KL}(p(D|M)||p(D|M'))$は,
0に近いほど真のモデルに近そうだということにする.

\subsection{エビデンス関数の評価の式変形}
$A=\alpha I + \beta \trans{\Phi}\Phi$とおくと
\begin{eqnarray*}
E(w) &=& \frac{\beta}{2}||t-\Phi w||^2+\frac{\alpha}{2}\trans{w}w\\
     &=& \frac{1}{2}\trans{w}(\alpha I + \beta \trans{\Phi}\Phi)w-\beta \trans{t}\Phi w+\frac{\beta}{2}||t||^2\\
     &=& \frac{1}{2}\trans{w}Aw-\beta \trans{w}\trans{\Phi} t+\frac{\beta}{2}||t||^2.
\end{eqnarray*}
ここで一般に対称行列$A$とベクトル$w$, $m$について
$$
\frac{1}{2}\trans{(w-m)}A(w-m)=\frac{1}{2}\trans{w}Aw-\trans{w}Am+\frac{1}{2}\trans{m}Am.
$$
この関数は$w=m$のとき最小値0をとる.
二つを比較することで$E(w)$は$\beta\trans{\Phi}t=Am$, つまり
$$
w=m_N=\beta A^{-1}\trans{\Phi}t
$$
のとき最小となる. 最小値は元の$E(w)$の式に$w=m_N$を代入すれば得られ,
$$
E(m_N)=\frac{\beta}{2}||t-\Phi m_N||^2+\frac{\alpha}{2}\trans{m_N} m_N.
$$
つまり
$$
E(w)=\frac{1}{2}\trans{(w-m_N)}A{(w-m_N)} + E(m_N)
$$
と平方完成できる.

よって
\begin{eqnarray*}
E(w) &=& \int \exp(-E(w))\,dw \\
     &=& \exp(-E(m_N)) \int \exp(-\frac{1}{2}\trans{(w-m_N)}A{(w-m_N)})\,dw\\
     &=& \exp(-E(m_N)) (2\pi)^{M/2} |A|^{-1/2}.
\end{eqnarray*}

従って
\begin{eqnarray}\label{log_evidence}
\log p(\bm{t}|\alpha,\beta)
 &=& (N/2) \log(\frac{\beta}{2\pi}) + (M/2) \log(\frac{\alpha}{2\pi}) \log (\int \exp(-E(w))\,dw)\\
 &=& (M/2)\log \alpha + \frac{N}{2} \log \beta - E(m_N) - \frac{1}{2}\log |A| - \frac{N}{2} \log (2\pi).
\end{eqnarray}


\subsection{ヘッセ行列}
$x$が$n$次縦ベクトルのとき, $y=f(x)$における2階微分の$n$次正方行列
$$
H(f)=(\difff{x_i}{x_j}f(x))
$$
をヘッセ行列という.
通常偏微分は可換なので, これは対称行列である.

1階微分の行列（ヤコビ行列）の行列式はその点の付近の拡大率を表していた.
ヘッセ行列はその点の付近の関数の形を表す.
たとえば正定値な場合は極小, 固有値が全て負の場合は極大, 固有値が正と負の両方の場合は鞍点となる.

$f=x^2-y^2$, $g=x^2+y^2$というグラフを見てみよう.
図\ref{x2my2}は原点で鞍点, 図\ref{x2py2}は原点で極小である.
それぞれヘッセ行列は
$$
H(f)=\matt{2}{0}{0}{-2},
$$
$$
H(g)=\matt{2}{0}{0}{2}
$$
となり, ヘッセ行列が原点での形に対応していることが分かる.
\begin{figure}[ht]
 \begin{minipage}{0.5\hsize}
  \begin{center}
   \includegraphics[scale=0.3]{x2my2.ps}
  \end{center}
  \caption{$f=x^2-y^2$}
  \label{x2my2}
 \end{minipage}
 \begin{minipage}{0.5\hsize}
  \begin{center}
   \includegraphics[scale=0.3]{x2py2.ps}
  \end{center}
  \caption{$g=x^2+y^2$}
  \label{x2py2}
 \end{minipage}
\end{figure}

\subsection{エビデンス関数の最大化の式変形}
行列$\beta\trans{\Phi}\Phi$をある行列$P$で対角化する.
$$
P^{-1} (\beta\trans{\Phi}\Phi) P = \diag(\lambda_1, \ldots, \lambda_M).
$$
すると行列$A=\alpha I + \beta\trans{\Phi}\Phi$も同じ$P$で対角化できて
$$
P^{-1} A P = \diag(\alpha + \lambda_1, \ldots, \alpha + \lambda_M).
$$
よって
$$
|A|=\prod_{i=1}^M(\lambda_i + \alpha)
$$
となる. $\alpha$で微分すると
$$
\diff{\alpha}\log |A|=\sum_{i=1}^M \frac{1}{\lambda_i + \alpha}.
$$
式\ref{log_evidence}を$\alpha$で微分すると
$$
\diff{\alpha} \log p(\bm{t}|\alpha,\beta)=\frac{M}{2\alpha}-\frac{1}{2}\trans{m_N}m_N - \frac{1}{2}\sum \frac{1}{\lambda_i + \alpha}=0.
$$
よって
$$
\alpha \trans{m_N}m_N = M-\sum_{i=1}^M \frac{\alpha}{\lambda_i + \alpha}=\sum_{i=1}^M \frac{\lambda_i}{\lambda_i + \alpha}.
$$
これを$\gamma$とおくと
$$
\alpha=\frac{\gamma}{\trans{m_N}m_N}.
$$
ただし, $m_N$は陰に$\alpha$に依存しているのでこれは実は$\alpha$を含む方程式である.

$\beta$についても同様にしてみる. $\beta\trans{\Phi}\Phi$の固有値が$\lambda_i$だから$\lambda_i$は$\beta$に比例する.
つまり微分が比例係数に等しい.
$$
\diff{\beta} \lambda_i = \lambda_i/\beta.
$$
よって
$$
\diff{\beta} \log |A| = \sum \frac{\lambda_i/\beta}{\lambda_i + \alpha}=\frac{\gamma}{\beta}.
$$
式\ref{log_evidence}を$\beta$で微分すると
$$
\frac{N}{2\beta} - \frac{1}{2}||\bm{t}-\Phi m_N||^2-\frac{\gamma}{2\beta}=0.
$$
よって
$$
\frac{1}{\beta}=\frac{1}{N-\gamma}||\bm{t}-\Phi m_N||^2.
$$

\subsection{パラメータの関係}
パラメータがたくさんでてきたのでそれらの関係を見直してみよう.
まず線形基底モデルを考えた. $\phi(x)$を$M$個の基底関数からなるベクトルとする.
$x$は観測値であり,
$$
y(x,w)=\trans{w}\phi(x)
$$
とした. $\bm{t}$を観測値に対する目標値で, それは$x$によらずに精度パラメータ$\beta$に従うガウス分布とした.
$$
p(\bm{t}|w,\beta)=\calN(t|y(x,w),\beta^{-1}).
$$
ベイズ的に扱うために$w$に関して事前確率分布を与えたい. 上式が$w$に関する2次関数なので,
共役事前分布としてハイパーパラメータ$\alpha$を導入し,
$$
p(w|\alpha)=\calN(w|0, \alpha^{-1}I)
$$
を仮定した. そうすることで事後分布は
$$
p(w|t)=\calN(w|m_N,S_N)
$$
の形（ただし, $m_N=\beta S_N\trans{\Phi}t$, $S_N^{-1}=\alpha I + \beta \trans{\Phi}\Phi$）になった.

さて, ここで$\alpha$, $\beta$はハイパーパラメータではあるが, 事前分布を入れて確率変数的に扱いたい.
その上で最尤推定の手法を用いて実際のデータから値を決めるという枠組みを経験ベイズという.
そのとき$t$の予測分布は
$$
p(t|\bm{t})=\int p(t|w,\beta)p(w|\bm{t},\alpha,\beta)p(\alpha,\beta|\bm{t})\,dwd\alpha d\beta
$$
となる. とはいえ, そのまま扱うのは難しいのでまずデータが十分たくさんあるとき, $\alpha$, $\beta$は殆ど固定値,
つまり$\alpha$,$\beta$の分布はある特定の値$\hat{\alpha}$, $\hat{\beta}$にデルタ関数的に近づくと仮定しよう.
$$
p(\alpha,\beta|\bm{t}) \sim \delta_{\alpha,\hat{\alpha}} \delta_{\beta,\hat{\beta}}.
$$
そうすると
$$
p(t|\bm{t}) \sim \int p(t|w,\hat{\beta})p(w|\bm{t},\hat{\alpha},\hat{\beta})\,dw
$$
となり予測分布は$\hat{\alpha}$, $\hat{\beta}$を求めればよいということになる.

次に$\alpha$, $\beta$を求める方法を考える. ベイズの定理から
$$
p(\alpha,\beta|\bm{t}) \propto p(\bm{t}|\alpha,\beta)p(\alpha,\beta)
$$
となる. ここで$p(\alpha,\beta)$はほぼ平坦, つまり$\alpha$, $\beta$の値はどれも同じぐらいの可能性があるという仮定を置く.

そうすると事後分布を最大化する$\alpha$, $\beta$を求める最尤推定の問題は, 尤度関数を最大化する問題に近似できる.
この尤度関数をエビデンスといい, この手法をエビデンス近似という.
そして, $p(\bm{t}|\alpha,\beta)$を最大化するための$\alpha$, $\beta$の関係式を求めたのが前節であった.

以上のパラメータの関係を図\ref{para-dep}に示した.
実際には, 初期値$\alpha$, $\beta$を適当に決め, この図に従って計算して新しい$\alpha$, $\beta$を求めたあと再度繰り返す.
それが収束すればその値を採用する. ここではその収束性については議論しない.
\begin{figure}[ht]
 \begin{minipage}{1\hsize}
  \begin{center}
%   \includegraphics[bb=0 0 360 270]{para-dep.pdf}
   \includegraphics[bb=0 0 260 220]{para-dep.pdf}
  \end{center}
  \caption{$\alpha$,$x$,$\phi$,$t$,$\beta$の関係図}
  \label{para-dep}
 \end{minipage}
\end{figure}
\end{document}
